{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kateMisis/misisexam/blob/main/%D0%9A%D1%80%D0%B8%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D0%B3%D1%80%D1%83%D0%BF%D0%BF%D1%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "eTzI9aBq1YDQ",
        "outputId": "79e0f4d0-c2f9-470f-d370-24ef71b0c915"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/population_2023_projected.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2723775956.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpop_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Reading csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mpop_readlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Read the file line by line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpop_readlines\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Go through the lines we've read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/population_2023_projected.csv'"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import networkx as nx\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "from itertools import groupby\n",
        "import copy\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "population_URL = 'https://drive.google.com/uc?export=download&id=<1JNZ1jgEXVWxNreJU6HHA-QfPiTrihXpd>'\n",
        "FAOSTAT_min_dietary_requirement_2014_2023_URL = 'https://drive.google.com/uc?export=download&id=<1JNZ1jgEXVWxNreJU6HHA-QfPiTrihXpd>'\n",
        "food_supply_URL = 'https://drive.google.com/uc?export=download&id=<1JNZ1jgEXVWxNreJU6HHA-QfPiTrihXpd>'\n",
        "trade_URL = 'https://drive.google.com/uc?export=download&id=<1JNZ1jgEXVWxNreJU6HHA-QfPiTrihXpd>'\n",
        "\n",
        "\n",
        "population = '/content/drive/My Drive/population_2023_projected.csv'\n",
        "FAOSTAT_min_dietary_requirement_2014_2023 = '/content/drive/My Drive/FAOSTAT_min_dietary_requirement_2014_2023-2.csv'\n",
        "food_supply = '/content/drive/My Drive/food_supply.csv'\n",
        "trade = '/content/drive/My Drive/trade_all_w1.csv'\n",
        "\n",
        "\n",
        "pop = dict()\n",
        "pop_file = open(population, 'r') #Reading csv file\n",
        "pop_readlines = pop_file.readlines() #Read the file line by line\n",
        "for k in pop_readlines: #Go through the lines we've read\n",
        "    c = k.split(',') #Separate these lines by the separator \",\"\n",
        "    if k != pop_readlines[0]:\n",
        "      pop[c[2]] = int(c[15])\n",
        "\n",
        "FAO_countries = set()\n",
        "\n",
        "FAO_min_data = dict() #A data array network_data is created, which will store the edges of our network (vertex1, vertex2, weight)\n",
        "FAO_min_file = open(FAOSTAT_min_dietary_requirement_2014_2023, 'r') #Reading csv file\n",
        "FAO_min_readlines = FAO_min_file.readlines() #Read the file line by line\n",
        "for k in FAO_min_readlines: #Go through the lines we've read\n",
        "    c = k.split(',') #Separate these lines by the separator \",\"\n",
        "    if (k != FAO_min_readlines[0]) and (c[8] == '2023'):\n",
        "        FAO_countries.add(c[3])\n",
        "        FAO_min_data[c[3]] = float(c[11])\n",
        "\n",
        "quota = dict()\n",
        "q_data = [] #A data array network_data is created, which will store the edges of our network (vertex1, vertex2, weight)\n",
        "q_file = open(food_supply, 'r') #Reading csv file\n",
        "q_readlines = q_file.readlines() #Read the file line by line\n",
        "for k in q_readlines: #Go through the lines we've read\n",
        "    c = k.split(',') #Separate these lines by the separator \",\"\n",
        "    if k != q_readlines[0]:\n",
        "        if c[3] in FAO_countries:\n",
        "            quota[c[3]] = float(((float(c[19])-FAO_min_data[c[3]])*pop[c[3]])*365) #Write data on the quotas of the considered vertices into the dictionary\n",
        "            q_data.append((c[3], ((float(c[19])-FAO_min_data[c[3]])*pop[c[3]])*365)) #Write the data of the line edge through \",\" into a tuple and write it into the network_data array\n",
        "\n",
        "n_data = [] #A data array network_data is created, which will store the edges of our network (vertex1, vertex2, weight)\n",
        "n_file = open(trade, 'r') #Reading csv file\n",
        "n_readlines = n_file.readlines() #Read the file line by line\n",
        "for k in n_readlines: #Go through the lines we've read\n",
        "    c = k.split(',') #Separate these lines by the separator \",\"\n",
        "    if (k != n_readlines[0]) and (c[0] == '2023'):\n",
        "        if (c[1] in FAO_countries) and (c[2] in FAO_countries):\n",
        "            n_data.append((c[1], c[2], float(c[3]))) #Write the data of the line edge through \",\" into a tuple and write it into the network_data array\n",
        "\n",
        "# Network creation\n",
        "Network = nx.DiGraph() #An empty graph is created for further creation of a directed graph.\n",
        "Network.add_weighted_edges_from(n_data) # Directed edges with weights are added to the empty graph\n",
        "\n",
        "Сountry = input('Введите название страны, чьи критические группы Вы желаете узнать: ')\n",
        "\n",
        "k_value = 3\n",
        "\n",
        "def In_degree (G):\n",
        "  In_degree = dict() #A dictionary In_degree is created, which will store for each vertex its In-degree value.\n",
        "  for j in nx.nodes(G): #We go through the list of vertices of a given network G\n",
        "      in_degree = 0 #For each vertex, a variable in_degree is created, which is initially equal to 0\n",
        "      for n in G.predecessors(j): #For a given vertex j, we iterate through the list of vertices from which edges go to j\n",
        "        in_degree += float(G[n][j]['weight']) #For each such vertex n, we add to the variable in_degree the weight of the edge from n to j\n",
        "      In_degree[j] = in_degree #We write the value of the variable In_degree for the vertex j into the dictionary In_degree\n",
        "  return In_degree\n",
        "\n",
        "Crtitcal_groups = [] #For each vertex j, it is created a list that will contain the critical groups for vertex j\n",
        "for k in range(min(k_value, Network.in_degree(Сountry))): #Set the number of vertices that can form a critical group\n",
        "  if In_degree(Network)[Сountry] != 0 and (Сountry in quota): #Test for zero centrality In_degree\n",
        "    for elem in combinations(Network.predecessors(Сountry), k+1): #Go through all possible combinations of elem from k+1 vertices from which an edge goes to vertex j\n",
        "      loc_nodes = [] #For each vertex, we create an array loc_nodes, which will contain the vertices from the critical group under consideration.\n",
        "      loc_nodes.append(Сountry) #Add node j to the loc_nodes array\n",
        "      for l in elem: #We go through all possible combinations of k+1 vertices\n",
        "        loc_nodes.append(l) #Add each combination to the loc_nodes array\n",
        "        G_lokal = nx.subgraph(Network, loc_nodes) #Create a subgraph G_lokal of a given network G on the nodes from the loc_nodes array\n",
        "        if In_degree(G_lokal)[Сountry] >= quota[Сountry]: #We set a check whether the In-degree for the considered vertex j in the subgraph G_lokal is greater than the given quota for the vertex j\n",
        "          Crtitcal_groups.append(elem) # In the array Crtitcal_groups we add the corresponding combination of elem\n",
        "print(Сountry, 'заивисит от следующих критических групп: ')\n",
        "for k in Crtitcal_groups:\n",
        "  print(Сountry, 'зависит от группы состоящей из ', k[0], ',', k[1], 'и', k[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Crtitcal_groups)"
      ],
      "metadata": {
        "id": "4saYcU1U1trn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}